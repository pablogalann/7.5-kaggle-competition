# Competici칩n de Kaggle para Bootcamp de Data OCT'22:


## 游늳OBJECTIVO

- Preparar los datos para los diversos modelos (proceso emp칤rico) 
- Entrenar y Testear modelos de Machine Learning
- Subir los resultados con el mejor modelo entrenado de Machine Learning
- Hacer pull request con la presentaci칩n en la carpeta de 'PPTS' 
- Crear repo propio del proyecto (issue)


## 游 CARGA Y ANALISIS DE DATOS

1췈 Importaci칩n de datos a partir de los archivos Salaries_data.csv y Testeo.csv

2췈 Exploracion, limpieza y transformaci칩n:
      df.type() > Observamos columnas de tipo categorico: 'experience_level','employment_type','job_title','employee_residence','company_location, 'company_size'. 
      
3췈 Con el fin de nuestro modelo predictivo funcione, debemos convertir todos los datos en categoricos en numericos:
    Utilizaci칩n de 2 metodos:
    - LabelEncoder(), Transforma cada uno de los datos categoricos en etiquetas numericas asociados a su peso dentro del df.
    - pd.get_dummies(), Transforma cada uno de los datos en etiquetas numericas binarias, entre 0 y 1.
    
* IMPORTANTE. Concatenar en un solo df, los dos dataframe que tenemos, para que el proceso de transformaci칩n a tipo numerico sea homogeneo

## 游녤 DETERMINAMOS X y Y

1췈 Una vez todos los datos de nuestro dataframe son numericos, determinamos la X y la y, con el fin de hacer una predici칩n sobre y.
2췈 Spliteamos x e y, para obtener un porcentaje de entreno y otro de test. 
    'X_train, X_test, y_train, y_test = tts(X, y, train_size=0.8, test_size=0.2, random_state=22)'
    
## 游뱄 ELEGIMOS EL MODELO DE MACHINE LEARNING.

1췈 Utilizamos lazyregressor para tener una aproximacion de los modelos m치s optimos para nuestro caso.
    'reg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None)' 

2췈 GradientBoostingRegressor                                   
   RandomForestRegressor                                           
   BaggingRegressor
   
   Estos 3 modelos son los que previsiblemente nos arrojan un error m치s bajo, en pasos posteriores comprobaremos empiricamente
   cierto
   
## 游 RAMDOM FOREST REGRESSOR.
from sklearn.ensemble import RandomForestRegressor as Rfr

1췈 Iniciamos modelo; rfr = Rfr()   
2췈 Entrenamos modelo; rfr.fit(X_train, y_train)
3췈 Predecimos modelo; y_pred_train=rfr.predict(X_testeo) +++++ UTILIZAMOS LA MUESTRA DE TESTEO KAGGLE+++++++ len=107
4췈 Calculamos RMSE y RSE.
   'MSE = mean_squared_error(y_train, y_pred_train)'
   'RMSE = math.sqrt(MSE)'


## 游꺍 GBR.
from sklearn.ensemble import GradientBoostingRegressor 

1췈 Iniciamos modelo; boo = GradientBoostingRegressor()   
2췈 Entrenamos modelo; boo.fit(X_train, y_train)
3췈 Predecimos modelo; y_pred_train=boo.predict(X_testeo) +++++ UTILIZAMOS LA MUESTRA DE TESTEO KAGGLE+++++++ len=107
4췈 Calculamos RMSE y RSE.
   'MSE = mean_squared_error(y_train, y_pred_train)'
   'RMSE = math.sqrt(MSE)'

## 游볞 ELEGIMOS LA MUESTRA MAS FAVORABLE Y LA CARGAMOS EN KAGGLE.

